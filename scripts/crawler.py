import requests
from bs4 import BeautifulSoup
import sqlite3
import json
import os
import re
from llm import CompletionExecutor
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(PROJECT_ROOT, 'data', 'scholar.db')

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def get_latest_article_no(table_name):
    """DBì—ì„œ ê°€ì¥ ìµœì‹  articleNo ê°€ì ¸ì˜¤ê¸°"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # idê°€ articleNoë¼ê³  ê°€ì •
    cursor.execute(f'SELECT MAX(id) as max_id FROM {table_name}')
    result = cursor.fetchone()
    conn.close()
    
    return result['max_id'] if result['max_id'] else 0

def crawl_notices(url, notice_type="ì¥í•™ê¸ˆ"):
    """ì„±ê· ê´€ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§"""
    print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘ ({notice_type}): {url}")
    
    try:
        # GET ìš”ì²­
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 10ê°œì˜ dl íƒœê·¸ ì°¾ê¸°
        content_wraps = soup.find_all('dl', class_='board-list-content-wrap', limit=10)
        
        if not content_wraps:
            print("âŒ board-list-content-wrapì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ“‹ {len(content_wraps)}ê°œì˜ ê³µì§€ì‚¬í•­ í•­ëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # ê° dlì—ì„œ a íƒœê·¸ ì°¾ê¸°
        links = []
        for wrap in content_wraps:
            a_tag = wrap.find('a', href=True)
            if a_tag:
                links.append(a_tag)
        
        notices = []
        seen_ids = set()  # ì¤‘ë³µ ì²´í¬ìš©
        
        for link in links:
            href = link.get('href', '')
            
            # articleNo ì¶”ì¶œ
            if 'articleNo=' in href:
                article_no = href.split('articleNo=')[1].split('&')[0]
                article_no = int(article_no)
                
                # ì¤‘ë³µ ì²´í¬
                if article_no in seen_ids:
                    print(f"âš ï¸  í¬ë¡¤ë§ ì¤‘ë³µ ë°œê²¬: {article_no}")
                    continue
                
                seen_ids.add(article_no)
                
                # ì œëª© ì¶”ì¶œ
                title = link.get_text(strip=True)
                
                notices.append({
                    'article_no': article_no,
                    'title': title,
                    'url': f"https://www.skku.edu/skku/campus/skk_comm/notice06.do?mode=view&articleNo={article_no}"
                })
        
        print(f"âœ… {len(notices)}ê°œì˜ ê³µì§€ì‚¬í•­ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return notices
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []

def get_notice_detail(article_no):
    """ê³µì§€ì‚¬í•­ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://www.skku.edu/skku/campus/skk_comm/notice06.do?mode=view&articleNo={article_no}"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        content_div = soup.find('pre')
        title = soup.find('em', class_='ellipsis').get_text(strip=True)

        if content_div and title:
            content = content_div.get_text(strip=True)
            return content, title
        else:
            return "ìƒì„¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        print(f"âŒ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ (articleNo={article_no}): {e}")
        return ""

def save_to_db(notices, table_name='scholarships'):
    """DBì— ì €ì¥ (ì¤‘ë³µ ì²´í¬)"""
    if not notices:
        print("ì €ì¥í•  ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    host = os.getenv('CLOVA_STUDIO_HOST')
    api_key = os.getenv('CLOVA_STUDIO_API_KEY')
    request_id = os.getenv('CLOVA_STUDIO_REQUEST_ID')

    completion_executor = CompletionExecutor(
        host=host,
        api_key=api_key,
        request_id=request_id
    )
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # í˜„ì¬ DBì˜ ìµœì‹  articleNo
    latest_id = get_latest_article_no(table_name)
    print(f"ğŸ“Š DB ìµœì‹  ID: {latest_id}")
    
    new_count = 0
    
    for notice in notices:
        article_no = notice['article_no']
        
        # ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if article_no <= latest_id:
            print(f"â­ï¸  ê±´ë„ˆë›°ê¸° (ê¸°ì¡´ ë°ì´í„°): {article_no} - {notice['title']}")
            continue
        
        
        print(f"ğŸ†• ìƒˆ ê³µì§€: {article_no} - {notice['title']}")
        
        # ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        full_text, title = get_notice_detail(article_no)
        
        # ì œì–´ ë¬¸ì ì œê±° (JSON íŒŒì‹± ì—ëŸ¬ ë°©ì§€)
        full_text_filtered = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', full_text)
        
        # DBì— ì‚½ì…
        try:
            result = completion_executor.execute(full_text_filtered)
            
            # í•„ìˆ˜ í•„ë“œ ê¸°ë³¸ê°’ ì²˜ë¦¬
            target_audience = result.get('target_audience') or "ê³µì§€ì‚¬í•­ ì°¸ì¡°"
            if isinstance(target_audience, list):
                target_audience = '\n'.join(f"- {item}" for item in target_audience)
            
            organizer = result.get('organizer') or "ë¯¸ì§€ì •"
            
            deadline = result.get('schedule', {}).get('deadline') or "ë¯¸ì •"
            
            selection_date = result.get('schedule', {}).get('selection_date')
            
            benefits = result.get('benefits') or "ê³µì§€ì‚¬í•­ ì°¸ì¡°"
            if isinstance(benefits, list):
                benefits = '\n'.join(f"- {item}" for item in benefits)
            
            category = result.get('category') or "ê¸°íƒ€"
            
            cursor.execute(f'''
                INSERT INTO {table_name} 
                (id, target_audience, organizer, deadline, selection_date, benefit, category, title, full_text)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article_no,
                target_audience,
                organizer,
                deadline,
                selection_date,
                benefits,
                category,
                title,
                full_text
            ))
            new_count += 1
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {article_no}")
            
        except sqlite3.IntegrityError as e:
            print(f"âš ï¸  IntegrityError (articleNo={article_no}): {e}")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ (articleNo={article_no}): {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {full_text[:200]}...")
        except KeyError as e:
            print(f"âŒ LLM ì‘ë‹µ í‚¤ ëˆ„ë½ (articleNo={article_no}): {e}")
            print(f"ì‘ë‹µ: {result}")
        except Exception as e:
            print(f"âŒ ì €ì¥ ì˜¤ë¥˜ (articleNo={article_no}): {e}")
            print(f"íƒ€ì…: {type(e).__name__}")
    
    conn.commit()
    conn.close()
    
    print(f"\nğŸ“¥ ì´ {new_count}ê°œì˜ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ“ ì„±ê· ê´€ëŒ€ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬")
    print("=" * 60)
    
    # 1. ì¥í•™ê¸ˆ ê³µì§€ í¬ë¡¤ë§ (notice06)
    print("\n" + "=" * 60)
    print("ğŸ“š ì¥í•™ê¸ˆ/ëª¨ì§‘ ê³µì§€ í¬ë¡¤ë§")
    print("=" * 60)
    scholarship_url = "https://www.skku.edu/skku/campus/skk_comm/notice06.do"
    scholarship_notices = crawl_notices(scholarship_url, "ì¥í•™ê¸ˆ/ëª¨ì§‘")
    
    # 2. ì±„ìš© ê³µê³  í¬ë¡¤ë§ (notice07)
    print("\n" + "=" * 60)
    print("ğŸ’¼ ì±„ìš©/ì·¨ì—… ê³µê³  í¬ë¡¤ë§")
    print("=" * 60)
    job_url = "https://www.skku.edu/skku/campus/skk_comm/notice07.do"
    job_notices = crawl_notices(job_url, "ì±„ìš©/ì·¨ì—…")
    
    # ê²°ê³¼ ì¶œë ¥
    total_count = 0
    
    if scholarship_notices:
        print("\nğŸ“‹ ì¥í•™ê¸ˆ/ëª¨ì§‘ í¬ë¡¤ë§ ê²°ê³¼:")
        for i, notice in enumerate(scholarship_notices, 1):
            print(f"{i}. [{notice['article_no']}] {notice['title']}")
        total_count += len(scholarship_notices)
    
    if job_notices:
        print("\nğŸ“‹ ì±„ìš©/ì·¨ì—… í¬ë¡¤ë§ ê²°ê³¼:")
        for i, notice in enumerate(job_notices, 1):
            print(f"{i}. [{notice['article_no']}] {notice['title']}")
        total_count += len(job_notices)
    
    if total_count > 0:
        # DBì— ìë™ ì €ì¥
        if scholarship_notices:
            print("\nğŸ’¾ ì¥í•™ê¸ˆ ë°ì´í„° ì €ì¥ ì¤‘...")
            save_to_db(scholarship_notices, table_name='scholarships')
        
        if job_notices:
            print("\nğŸ’¾ ì±„ìš© ê³µê³  ë°ì´í„° ì €ì¥ ì¤‘...")
            save_to_db(job_notices, table_name='jobs')
    else:
        print("âš ï¸  í¬ë¡¤ë§ëœ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)
    print("âœ… í¬ë¡¤ëŸ¬ ì¢…ë£Œ")
    print("=" * 60)

if __name__ == "__main__":
    main()
